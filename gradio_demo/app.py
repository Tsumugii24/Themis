import sys
import os  # ç”¨äºæ“ä½œç³»ç»Ÿç›¸å…³çš„æ“ä½œï¼Œä¾‹å¦‚è¯»å–ç¯å¢ƒå˜é‡
import io  # ç”¨äºå¤„ç†æµå¼æ•°æ®ï¼ˆä¾‹å¦‚æ–‡ä»¶æµï¼‰
import gradio as gr
from dotenv import load_dotenv, find_dotenv
from llm_sdk.call_llm import get_completion
from database.create_db import create_db_info
from qa_chain.chat_qa_chain_self import Chat_QA_Chain_Self
from qa_chain.qa_chain_self import QA_Chain_Self

load_dotenv(find_dotenv())
LLM_MODEL_DICT = {
    "openai": ["gpt-3.5-turbo", "gpt-3.5-turbo-16k-0613", "gpt-3.5-turbo-0613", "gpt-4", "gpt-4-32k"],
    "baidu": ["ERNIE-4.0-8K", "ERNIE-3.5-8K", "ERNIE-3.5-8K-0205"],
    "spark": ["Spark-3.5", "Spark-3.0"],
    "zhipu": ["glm-4", "glm-4v", "glm-3-turbo"]
}

LLM_MODEL_LIST = sum(list(LLM_MODEL_DICT.values()), [])
INIT_LLM = "glm-4"
EMBEDDING_MODEL_LIST = ['openai', 'zhipu', 'm3e', 'gte']
INIT_EMBEDDING_MODEL = "openai"
# å…³äºæ³•å¾‹é—®é¢˜çš„æœ¬åœ°çŸ¥è¯†åº“å’Œå‘é‡æ•°æ®åº“çš„é»˜è®¤è·¯å¾„
DEFAULT_DB_PATH = "../../database/laws_knowledgebases"
DEFAULT_PERSIST_PATH = "../../database/chromadb/laws_vectordb_openai"
EXAMPLE_AVATAR_PATH = "logo.png"
EXTRA_AVATAR_PATH = "logo.png"
EXAMPLE_LOGO_PATH = "logo.png"
EXTRA_LOGO_PATH = "logo.png"


def get_model_by_platform(platform):
    return LLM_MODEL_DICT.get(platform, "")


class Model_Center():
    """
    å­˜å‚¨é—®ç­” Chain çš„å¯¹è±¡ 

    - chat_qa_chain_self: ä»¥ (model, embedding) ä¸ºé”®å­˜å‚¨çš„å¸¦å†å²è®°å½•çš„é—®ç­”é“¾ã€‚
    - qa_chain_self: ä»¥ (model, embedding) ä¸ºé”®å­˜å‚¨çš„ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾ã€‚
    """

    def __init__(self):
        self.chat_qa_chain_self = {}
        self.qa_chain_self = {}

    def chat_qa_chain_self_answer(self, question: str, chat_history: list = [], model: str = "openai",
                                  embedding: str = "openai", temperature: float = 0.0, top_k: int = 4,
                                  history_len: int = 3, file_path: str = DEFAULT_DB_PATH,
                                  persist_path: str = DEFAULT_PERSIST_PATH):
        """
        è°ƒç”¨å¸¦å†å²è®°å½•çš„é—®ç­”é“¾è¿›è¡Œå›ç­”
        """
        if question == None or len(question) < 1:
            return "", chat_history
        try:
            if (model, embedding) not in self.chat_qa_chain_self:
                self.chat_qa_chain_self[(model, embedding)] = Chat_QA_Chain_Self(model=model, temperature=temperature,
                                                                                 top_k=top_k, chat_history=chat_history,
                                                                                 file_path=file_path,
                                                                                 persist_path=persist_path,
                                                                                 embedding=embedding)
            chain = self.chat_qa_chain_self[(model, embedding)]
            return "", chain.answer(question=question, temperature=temperature, top_k=top_k)
        except Exception as e:
            return e, chat_history

    def qa_chain_self_answer(self, question: str, chat_history: list = [], model: str = "openai", embedding="openai",
                             temperature: float = 0.0, top_k: int = 4, file_path: str = DEFAULT_DB_PATH,
                             persist_path: str = DEFAULT_PERSIST_PATH):
        """
        è°ƒç”¨ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾è¿›è¡Œå›ç­”
        """
        if question == None or len(question) < 1:
            return "", chat_history
        try:
            if (model, embedding) not in self.qa_chain_self:
                self.qa_chain_self[(model, embedding)] = QA_Chain_Self(model=model, temperature=temperature,
                                                                       top_k=top_k, file_path=file_path,
                                                                       persist_path=persist_path, embedding=embedding)
            chain = self.qa_chain_self[(model, embedding)]
            chat_history.append(
                (question, chain.answer(question, temperature, top_k)))
            return "", chat_history
        except Exception as e:
            return e, chat_history

    def clear_history(self):
        if len(self.chat_qa_chain_self) > 0:
            for chain in self.chat_qa_chain_self.values():
                chain.clear_history()


def format_chat_prompt(message, chat_history):
    """
    è¯¥å‡½æ•°ç”¨äºæ ¼å¼åŒ–èŠå¤© promptã€‚

    å‚æ•°:
    message: å½“å‰çš„ç”¨æˆ·æ¶ˆæ¯ã€‚
    chat_history: èŠå¤©å†å²è®°å½•ã€‚

    è¿”å›:
    prompt: æ ¼å¼åŒ–åçš„ promptã€‚
    """
    # åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼Œç”¨äºå­˜æ”¾æ ¼å¼åŒ–åçš„èŠå¤© promptã€‚
    prompt = ""
    # éå†èŠå¤©å†å²è®°å½•ã€‚
    for turn in chat_history:
        # ä»èŠå¤©è®°å½•ä¸­æå–ç”¨æˆ·å’Œæœºå™¨äººçš„æ¶ˆæ¯ã€‚
        user_message, bot_message = turn
        # æ›´æ–° promptï¼ŒåŠ å…¥ç”¨æˆ·å’Œæœºå™¨äººçš„æ¶ˆæ¯ã€‚
        prompt = f"{prompt}\nUser: {user_message}\nAssistant: {bot_message}"
    # å°†å½“å‰çš„ç”¨æˆ·æ¶ˆæ¯ä¹ŸåŠ å…¥åˆ° promptä¸­ï¼Œå¹¶é¢„ç•™ä¸€ä¸ªä½ç½®ç»™æœºå™¨äººçš„å›å¤ã€‚
    prompt = f"{prompt}\nUser: {message}\nAssistant:"
    # è¿”å›æ ¼å¼åŒ–åçš„ promptã€‚
    return prompt


def respond(message, chat_history, llm, history_len=3, temperature=0.1, max_tokens=2048):
    """
    è¯¥å‡½æ•°ç”¨äºç”Ÿæˆæœºå™¨äººçš„å›å¤ã€‚

    å‚æ•°:
    message: å½“å‰çš„ç”¨æˆ·æ¶ˆæ¯ã€‚
    chat_history: èŠå¤©å†å²è®°å½•ã€‚

    è¿”å›:
    "": ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ²¡æœ‰å†…å®¹éœ€è¦æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Šï¼Œå¯ä»¥æ›¿æ¢ä¸ºçœŸæ­£çš„æœºå™¨äººå›å¤ã€‚
    chat_history: æ›´æ–°åçš„èŠå¤©å†å²è®°å½•
    """
    if message is None or len(message) < 1:
        return "", chat_history
    try:
        # é™åˆ¶ history çš„è®°å¿†é•¿åº¦
        chat_history = chat_history[-history_len:] if history_len > 0 else []
        # è°ƒç”¨ä¸Šé¢çš„å‡½æ•°ï¼Œå°†ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•æ ¼å¼åŒ–ä¸ºä¸€ä¸ª promptã€‚
        formatted_prompt = format_chat_prompt(message, chat_history)
        # ä½¿ç”¨llmå¯¹è±¡çš„predictæ–¹æ³•ç”Ÿæˆæœºå™¨äººçš„å›å¤ï¼ˆæ³¨æ„ï¼šllmå¯¹è±¡åœ¨æ­¤ä»£ç ä¸­å¹¶æœªå®šä¹‰ï¼‰ã€‚
        bot_message = get_completion(
            formatted_prompt, llm, temperature=temperature, max_tokens=max_tokens)
        # å°†ç”¨æˆ·çš„æ¶ˆæ¯å’Œæœºå™¨äººçš„å›å¤åŠ å…¥åˆ°èŠå¤©å†å²è®°å½•ä¸­ã€‚
        chat_history.append((message, bot_message))
        # è¿”å›ä¸€ä¸ªç©ºå­—ç¬¦ä¸²å’Œæ›´æ–°åçš„èŠå¤©å†å²è®°å½•ï¼ˆè¿™é‡Œçš„ç©ºå­—ç¬¦ä¸²å¯ä»¥æ›¿æ¢ä¸ºçœŸæ­£çš„æœºå™¨äººå›å¤ï¼Œå¦‚æœéœ€è¦æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Šï¼‰ã€‚
        return "", chat_history
    except Exception as e:
        return e, chat_history


model_center = Model_Center()

block = gr.Blocks()
with block as demo:
    with gr.Row(equal_height=True):
        gr.Image(value=EXAMPLE_LOGO_PATH, scale=1, min_width=10, show_label=False, show_download_button=False,
                 container=False)

        with gr.Column(scale=2):
            gr.Markdown("""<h1><center>ChatLaw</center></h1>
                <h3><center>Maintained by Tsumugii https://github.com/Tsumugii24/ChatLaw ğŸ˜Š</center></h3>
                """)
        gr.Image(value=EXTRA_LOGO_PATH, scale=1, min_width=10, show_label=False, show_download_button=False,
                 container=False)

    with gr.Row():
        with gr.Column(scale=4):
            chatbot = gr.Chatbot(height=400, show_copy_button=True, show_share_button=True,
                                 avatar_images=(EXAMPLE_AVATAR_PATH, EXTRA_AVATAR_PATH))
            # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
            msg = gr.Textbox(label="your question", placeholder="Type your message here...",)

            with gr.Row():
                # åˆ›å»ºæäº¤æŒ‰é’®ã€‚
                db_with_his_btn = gr.Button("Chat db with history")
                db_wo_his_btn = gr.Button("Chat db without history")
                llm_btn = gr.Button("Chat with llm")
            with gr.Row():
                # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                clear = gr.ClearButton(
                    components=[chatbot], value="Clear console")

        with gr.Column(scale=1):
            file = gr.File(label='knowledge base uploading', file_count='directory',
                           file_types=['.txt', '.md', '.docx', '.pdf'])  # todo .jpg .png .jpegçš„æ·»åŠ å’Œå¤„ç†
            with gr.Row():
                init_db = gr.Button("documents vectorization")
            model_argument = gr.Accordion("parameter configuration", open=False)
            with model_argument:
                temperature = gr.Slider(0,
                                        1,
                                        value=0.01,
                                        step=0.01,
                                        label="llm temperature",
                                        interactive=True)

                top_k = gr.Slider(1,
                                  10,
                                  value=3,
                                  step=1,
                                  label="vector db search top k",
                                  interactive=True)

                history_len = gr.Slider(0,
                                        5,
                                        value=3,
                                        step=1,
                                        label="history length",
                                        interactive=True)

            model_select = gr.Accordion("model selection", open=True)
            with model_select:
                llm = gr.Dropdown(
                    LLM_MODEL_LIST,
                    label="Large Language Model (ChatModel)",
                    value=INIT_LLM,
                    interactive=True)

                embeddings = gr.Dropdown(EMBEDDING_MODEL_LIST,
                                         label="Embedding Model",
                                         value=INIT_EMBEDDING_MODEL)

        # è®¾ç½®åˆå§‹åŒ–å‘é‡æ•°æ®åº“æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ create_db_info å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ–‡ä»¶å’Œå¸Œæœ›ä½¿ç”¨çš„ Embedding æ¨¡å‹ã€‚
        init_db.click(create_db_info,
                      inputs=[file, embeddings], outputs=[msg])

        # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ chat_qa_chain_self_answer å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
        db_with_his_btn.click(model_center.chat_qa_chain_self_answer, inputs=[
            msg, chatbot, llm, embeddings, temperature, top_k, history_len], outputs=[msg, chatbot])
        # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ qa_chain_self_answer å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
        db_wo_his_btn.click(model_center.qa_chain_self_answer, inputs=[
            msg, chatbot, llm, embeddings, temperature, top_k], outputs=[msg, chatbot])
        # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ respond å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
        llm_btn.click(respond, inputs=[
            msg, chatbot, llm, history_len, temperature], outputs=[msg, chatbot], show_progress="minimal")

        # è®¾ç½®æ–‡æœ¬æ¡†çš„æäº¤äº‹ä»¶ï¼ˆå³æŒ‰ä¸‹Enteré”®æ—¶ï¼‰ã€‚åŠŸèƒ½ä¸ä¸Šé¢çš„ llm_btn æŒ‰é’®ç‚¹å‡»äº‹ä»¶ç›¸åŒã€‚
        msg.submit(respond, inputs=[
            msg, chatbot, llm, history_len, temperature], outputs=[msg, chatbot], show_progress="hidden")
        # ç‚¹å‡»åæ¸…ç©ºåç«¯å­˜å‚¨çš„èŠå¤©è®°å½•
        clear.click(model_center.clear_history)
    gr.Markdown("""Tipsï¼š<br>
    1. You may upload your own directory of documents to create a knowledge base, otherwise we will use default knowledge base that comes with the project. <br>
    2. It may take a while to vectorize your documents, so please be patient. <br>
    3. If there is any error or abnormality while running, it will be displayed in the text input box, you can copy the info and open a github issue. <br>
    """)
# threads to consume the request
gr.close_all()
# å¯åŠ¨æ–°çš„ Gradio åº”ç”¨ï¼Œè®¾ç½®åˆ†äº«åŠŸèƒ½ä¸º Trueï¼Œå¹¶ä½¿ç”¨ç¯å¢ƒå˜é‡ PORT1 æŒ‡å®šæœåŠ¡å™¨ç«¯å£ã€‚
# demo.launch(share=True, server_port=int(os.environ['PORT1']))
# ç›´æ¥å¯åŠ¨
# todo ç¼–ç é—®é¢˜ gbk / ä½¿ç”¨ignore errorsä¹‹åè·¯å¾„è§£æå­˜åœ¨é—®é¢˜
demo.launch(inbrowser=True,  # è‡ªåŠ¨æ‰“å¼€é»˜è®¤æµè§ˆå™¨
            share=False,  # é¡¹ç›®æš‚ä¸å…±äº«ï¼Œå…¶ä»–è®¾å¤‡ç›®å‰ä¸èƒ½è®¿é—®
            favicon_path="C:\\Users\YUI\PycharmProjects\ChatLaw\doc\favicons\favicon.ico",  # ç½‘é¡µå›¾æ ‡
            show_error=True,  # åœ¨æµè§ˆå™¨æ§åˆ¶å°ä¸­æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            quiet=True,  # ç¦æ­¢å¤§å¤šæ•°æ‰“å°è¯­å¥
            )
